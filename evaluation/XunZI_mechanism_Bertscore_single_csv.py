import os
import gc
import numpy as np
import pandas as pd
from bert_score import score as bert_score
import torch
from transformers import AutoTokenizer

# =====================================================
# 1. é…ç½®åŒº â€”â€” ä½ åªè¦æ”¹è¿™é‡Œ
# =====================================================
data_dir = "/work/home/acssag9hf5/XunZi/XunZI-R/Metric/XunZI_independent_dataset"  # è¿™ä¸ªæ–‡ä»¶å¤¹
input_csv_name = "XunZiR_train_on_Disgenet.csv"  # ğŸ‘ˆ æ”¹æˆä½ è¦ç®—çš„é‚£ä¸ª csv æ–‡ä»¶å
input_csv_path = os.path.join(data_dir, input_csv_name)

ref_col = "Mechanism_answer"   # å‚è€ƒè§£é‡Šåˆ—å
resp_col = "response"          # æ¨¡å‹è¾“å‡ºåˆ—å

bert_model_path = "/work/home/acssag9hf5/XunZi/XunZI-R/Metric/Models/scibert_scivocab_uncased"

# è¾“å‡ºï¼šåœ¨åŸ csv çš„åŸºç¡€ä¸ŠåŠ ä¸Š BERTScoreï¼Œæ¯ä¸€è¡Œéƒ½æœ‰
output_csv_name = input_csv_name.replace(".csv", "_with_bertscore.csv")
output_csv_path = os.path.join(data_dir, output_csv_name)

# BERTScore åˆ†å—è®¾ç½®
BERT_CHUNK_SIZE   = 20000   # ä¸€æ¬¡æœ€å¤šå¤„ç† 2w æ¡æ ·æœ¬
BERT_BATCH_SIZE   = 128     # BERTScore å†…éƒ¨ batch_size

# =====================================================
# 2. é€‰æ‹©è®¾å¤‡ï¼šä¼˜å…ˆç”¨ GPU
# =====================================================
device = "cuda" if torch.cuda.is_available() else "cpu"
print(">>> BERTScore will run on device:", device)

# =====================================================
# 3. åŠ è½½ tokenizer
# =====================================================
print("Loading tokenizer...")
try:
    tokenizer = AutoTokenizer.from_pretrained(bert_model_path)
except Exception as e:
    print(f"Failed to load local tokenizer: {e}")
    print("Falling back to online SciBERT tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained("allenai/scibert_scivocab_uncased")


def smart_truncate_with_tokenizer(text, tokenizer, max_tokens=400):
    """
    ä½¿ç”¨ SciBERT tokenizer æˆªæ–­ï¼Œä¿è¯ subword é•¿åº¦ä¸è¶…è¿‡ max_tokensã€‚
    """
    if not isinstance(text, str) or not text.strip():
        return ""
    try:
        tokens = tokenizer.tokenize(text)
        if len(tokens) > max_tokens:
            tokens = tokens[:max_tokens]
        truncated = tokenizer.convert_tokens_to_string(tokens)
        return truncated.strip()
    except Exception:
        # å…œåº•ï¼šæŒ‰å•è¯æ•°æˆªæ–­
        words = text.split()
        return " ".join(words[:max_tokens // 2])


# =====================================================
# 4. è¯»å–å•ä¸ª csvï¼Œå‡†å¤‡ refs å’Œ hyps
# =====================================================
print(f"\n>>> Loading file: {input_csv_path}")
df = pd.read_csv(input_csv_path)
print("Columns:", df.columns.tolist())

if ref_col not in df.columns:
    raise KeyError(f"{input_csv_path} does not contain reference column '{ref_col}'")
if resp_col not in df.columns:
    raise KeyError(f"{input_csv_path} does not contain response column '{resp_col}'")

# å¡«å……ç©ºå€¼
df[ref_col] = df[ref_col].fillna("").astype(str)
df[resp_col] = df[resp_col].fillna("").astype(str)

# æˆªæ–­å‚è€ƒ & é¢„æµ‹æ–‡æœ¬
df["ref_trunc"] = df[ref_col].apply(
    lambda x: smart_truncate_with_tokenizer(x, tokenizer, max_tokens=400)
)
df["hyp_trunc"] = df[resp_col].apply(
    lambda x: smart_truncate_with_tokenizer(x, tokenizer, max_tokens=400)
)

# è¿‡æ»¤ï¼šå‚è€ƒæ–‡æœ¬å¤ªçŸ­çš„è¡Œå¯ä»¥é€‰æ‹©è¿‡æ»¤æ‰ï¼ˆå¯é€‰ï¼‰
valid_mask = df["ref_trunc"].str.len() > 5
valid_df = df[valid_mask].copy().reset_index(drop=True)

print(f"Total samples: {len(df)}, valid for BERTScore: {len(valid_df)}")

refs = valid_df["ref_trunc"].tolist()
hyps = valid_df["hyp_trunc"].tolist()
n_samples = len(refs)


# =====================================================
# 5. åˆ†å—è®¡ç®— BERTScore çš„å‡½æ•°
# =====================================================
def bertscore_chunked(hyps_list, refs_list, model_type, device="cuda",
                      batch_size=16, chunk_size=20000):
    """
    å¯¹é•¿åˆ—è¡¨åˆ†å—è®¡ç®— BERTScoreï¼Œé¿å…ä¸€æ¬¡æ€§å ç”¨è¿‡å¤šå†…å­˜ã€‚
    è¿”å›æ‹¼æ¥åçš„ P/R/F1 å¼ é‡ï¼ˆé•¿åº¦ == len(hyps_list)ï¼‰
    """
    assert len(hyps_list) == len(refs_list)
    n = len(hyps_list)
    all_P = []
    all_R = []
    all_F1 = []

    for start in range(0, n, chunk_size):
        end = min(start + chunk_size, n)
        hyps_chunk = hyps_list[start:end]
        refs_chunk = refs_list[start:end]

        print(f"  BERTScore chunk {start} : {end} ...")
        P, R, F1 = bert_score(
            hyps_chunk,
            refs_chunk,
            lang="en",
            model_type=model_type,
            num_layers=12,
            rescale_with_baseline=False,
            batch_size=batch_size,
            device=device,
        )
        all_P.append(P.cpu())
        all_R.append(R.cpu())
        all_F1.append(F1.cpu())

        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        del P, R, F1
        gc.collect()

    P_full = torch.cat(all_P, dim=0)
    R_full = torch.cat(all_R, dim=0)
    F1_full = torch.cat(all_F1, dim=0)

    return P_full, R_full, F1_full


# =====================================================
# 6. è®¡ç®—è¯¥ csv çš„ BERTScore
# =====================================================
print("Computing BERTScore (chunked) for this file...")

P_full, R_full, F1_full = bertscore_chunked(
    hyps_list=hyps,
    refs_list=refs,
    model_type=bert_model_path,
    device=device,
    batch_size=BERT_BATCH_SIZE,
    chunk_size=BERT_CHUNK_SIZE,
)

bert_P_mean  = float(P_full.mean())
bert_R_mean  = float(R_full.mean())
bert_F1_mean = float(F1_full.mean())

print(f"\nBERTScore P (mean):  {bert_P_mean:.4f}")
print(f"BERTScore R (mean):  {bert_R_mean:.4f}")
print(f"BERTScore F1 (mean): {bert_F1_mean:.4f}")

# =====================================================
# 7. æŠŠé€è¡Œ P_full/R_full/F1_full å†™å› DataFrame
#    æ³¨æ„è¦å¯¹é½ï¼šåªå¯¹ valid_mask == True çš„è¡Œå†™å…¥
# =====================================================
# åˆå§‹åŒ–ä¸º NaNï¼Œä¿è¯è¡Œæ•°ä¸åŸ df ä¸€è‡´
df["BERT_P"]  = np.nan
df["BERT_R"]  = np.nan
df["BERT_F1"] = np.nan

# å¯¹ valid_df çš„ç´¢å¼•åºåˆ—å†™å›å¯¹åº”åˆ†æ•°
valid_indices = df[valid_mask].index.tolist()  # åŸ df çš„è¡Œå·
df.loc[valid_indices, "BERT_P"]  = P_full.numpy()
df.loc[valid_indices, "BERT_R"]  = R_full.numpy()
df.loc[valid_indices, "BERT_F1"] = F1_full.numpy()

# =====================================================
# 8. ä¿å­˜å¸¦è¡Œçº§ BERTScore çš„æ–° csv
# =====================================================
df.to_csv(output_csv_path, index=False)
print(f"\nSaved file with per-sample BERTScore to:\n  {output_csv_path}")
